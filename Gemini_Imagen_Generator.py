import json
import requests
import base64
from io import BytesIO
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import torch
from typing import List, Dict, Optional, Tuple, Any
import re
import random
import time
import textwrap
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
import threading
import os
import configparser
import asyncio
import hashlib
from datetime import datetime
from functools import partial
from collections import OrderedDict
from requests.adapters import HTTPAdapter
from aiohttp import web

from server import PromptServer
import comfy.utils
import comfy.model_management

# å¯¼å…¥æ–°çš„æ—¥å¿—ç³»ç»Ÿ
try:
    from .logger import logger
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    import os
    # ç¡®ä¿å½“å‰ç›®å½•åœ¨ sys.path ä¸­
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)
    from logger import logger



def retry_with_backoff(tries=3, delay=2, backoff=2, retriable_exceptions=None):
    """
    æ™ºèƒ½é‡è¯•è£…é¥°å™¨ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿
    
    Args:
        tries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆåŒ…æ‹¬åˆæ¬¡å°è¯•ï¼‰
        delay: åˆå§‹å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        backoff: é€€é¿å€æ•°
        retriable_exceptions: å¯é‡è¯•çš„å¼‚å¸¸ç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç½‘ç»œç›¸å…³å¼‚å¸¸
    """
    if retriable_exceptions is None:
        # é»˜è®¤é‡è¯•å¯æ¢å¤çš„é”™è¯¯ï¼ˆ5xxã€ç½‘ç»œè¶…æ—¶ã€è¿æ¥ä¸­æ–­ï¼‰
        retriable_exceptions = (
            requests.exceptions.Timeout,
            requests.exceptions.ConnectionError,
            requests.exceptions.HTTPError,
            requests.exceptions.ChunkedEncodingError,  # å“åº”åˆ†å—ä¼ è¾“ä¸­æ–­
            requests.exceptions.RequestException,  # å…œåº•çš„ç½‘ç»œå¼‚å¸¸
            ConnectionResetError,  # è¿æ¥è¢«é‡ç½®
            BrokenPipeError,  # ç®¡é“ç ´è£‚
        )
    
    def decorator(func):
        from functools import wraps
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            mtries, mdelay = tries, delay
            
            for attempt in range(mtries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¯é‡è¯•çš„å¼‚å¸¸
                    is_retriable = False

                    # æ£€æŸ¥æ˜¯å¦æ˜¯5xxé”™è¯¯
                    if isinstance(e, Exception) and "APIè¿”å› 5" in str(e):
                        is_retriable = True
                    # æ£€æŸ¥è¿æ¥ä¸­æ–­ç›¸å…³é”™è¯¯(IncompleteReadç­‰)
                    elif "IncompleteRead" in str(type(e)) or "IncompleteRead" in str(e):
                        is_retriable = True
                    # æ£€æŸ¥å“åº”è¿‡æ—©ç»“æŸ
                    elif "Response ended prematurely" in str(e):
                        is_retriable = True
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é¢„å®šä¹‰çš„å¯é‡è¯•å¼‚å¸¸
                    elif isinstance(e, retriable_exceptions):
                        is_retriable = True
                    
                    # æœ€åä¸€æ¬¡å°è¯•æˆ–ä¸å¯é‡è¯•çš„é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    if attempt == mtries - 1 or not is_retriable:
                        raise

                    # æ‰“å°é‡è¯•ä¿¡æ¯
                    logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{mtries}): {str(e)[:100]}")
                    logger.info(f"ç­‰å¾… {mdelay:.1f}s åé‡è¯•...")

                    # ç­‰å¾…åé‡è¯•
                    time.sleep(mdelay)
                    mdelay *= backoff  # æŒ‡æ•°é€€é¿
            
        return wrapper
    return decorator


class BananaImageNode:
    """
    ComfyUIèŠ‚ç‚¹: NanoBananaå›¾åƒç”Ÿæˆï¼Œé€‚é…Geminiå…¼å®¹ç«¯ç‚¹
    æ”¯æŒä»config.iniè¯»å–API Key
    """

    DEFAULT_API_BASE_URL = "https://xinbaoapi.feng1994.xin"
    TOKENS_PER_RATE = 100000
    CURRENCY_PER_RATE = 0.20
    BASE_COST_PER_TOKEN = CURRENCY_PER_RATE / TOKENS_PER_RATE
    _BALANCE_CACHE: Dict[str, Dict[str, Any]] = {}
    _BALANCE_CACHE_LOCK = threading.Lock()
    _BALANCE_ROUTE_REGISTERED = False
    _BALANCE_ROUTE_TIMER: Optional[threading.Timer] = None
    _BALANCE_CACHE_TTL = 60.0
    _IMAGE_B64_CACHE: "OrderedDict[str, str]" = OrderedDict()
    _IMAGE_CACHE_LOCK = threading.Lock()
    _IMAGE_CACHE_SIZE = 16
    _ERROR_FONT_CACHE: Dict[int, ImageFont.ImageFont] = {}
    _SESSION: Optional[requests.Session] = None
    _SESSION_LOCK = threading.Lock()

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("images", "text")
    FUNCTION = "generate_images"
    OUTPUT_NODE = True
    CATEGORY = "image/ai_generation"

    @classmethod
    def _sanitize_api_key(cls, api_key: Optional[str]) -> Optional[str]:
        if not api_key:
            return None
        cleaned = api_key.strip()
        if not cleaned or cleaned == "your-api-key-here":
            return None
        return cleaned

    @staticmethod
    def _clamp_cost_factor(cost_factor: Optional[float]) -> float:
        if cost_factor is None:
            return 1.0
        try:
            value = float(cost_factor)
        except (TypeError, ValueError):
            return 1.0
        return max(0.0001, min(value, 100.0))

    @classmethod
    def _balance_cache_key(cls, api_base_url: str, api_key: str) -> str:
        normalized_url = (api_base_url or cls.DEFAULT_API_BASE_URL).rstrip("/").lower()
        digest = hashlib.sha256(api_key.encode("utf-8")).hexdigest()
        return f"{normalized_url}|{digest}"

    @classmethod
    def _tensor_cache_key(cls, tensor: Optional[torch.Tensor]) -> Optional[str]:
        if tensor is None:
            return None
        try:
            np_data = tensor.detach().cpu().numpy()
        except Exception:
            return None
        try:
            return hashlib.sha1(np_data.tobytes()).hexdigest()
        except Exception:
            return None

    @classmethod
    def _get_cached_image_b64(cls, cache_key: Optional[str]) -> Optional[str]:
        if not cache_key:
            return None
        with cls._IMAGE_CACHE_LOCK:
            value = cls._IMAGE_B64_CACHE.get(cache_key)
            if value is not None:
                cls._IMAGE_B64_CACHE.move_to_end(cache_key)
            return value

    @classmethod
    def _set_cached_image_b64(cls, cache_key: Optional[str], value: str) -> None:
        if not cache_key or not value:
            return
        with cls._IMAGE_CACHE_LOCK:
            cls._IMAGE_B64_CACHE[cache_key] = value
            cls._IMAGE_B64_CACHE.move_to_end(cache_key)
            while len(cls._IMAGE_B64_CACHE) > cls._IMAGE_CACHE_SIZE:
                cls._IMAGE_B64_CACHE.popitem(last=False)

    @classmethod
    def _store_balance_snapshot(cls, api_base_url: str, api_key: str, payload: Dict[str, Any]) -> None:
        sanitized_key = cls._sanitize_api_key(api_key)
        if not sanitized_key:
            return
        cache_key = cls._balance_cache_key(api_base_url, sanitized_key)
        snapshot = {
            "payload": payload,
            "fetched_at": time.time()
        }
        with cls._BALANCE_CACHE_LOCK:
            cls._BALANCE_CACHE[cache_key] = snapshot

    @classmethod
    def _get_balance_snapshot(cls, api_base_url: str, api_key: str) -> Optional[Dict[str, Any]]:
        sanitized_key = cls._sanitize_api_key(api_key)
        if not sanitized_key:
            return None
        cache_key = cls._balance_cache_key(api_base_url, sanitized_key)
        with cls._BALANCE_CACHE_LOCK:
            return cls._BALANCE_CACHE.get(cache_key)

    @classmethod
    def _snapshot_age_seconds(cls, snapshot: Optional[Dict[str, Any]]) -> Optional[float]:
        if not snapshot:
            return None
        fetched_at = snapshot.get("fetched_at")
        if not fetched_at:
            return None
        return max(0.0, time.time() - fetched_at)

    @classmethod
    def _is_balance_snapshot_stale(cls, snapshot: Optional[Dict[str, Any]]) -> bool:
        age = cls._snapshot_age_seconds(snapshot)
        if age is None:
            return True
        return age > cls._BALANCE_CACHE_TTL

    @classmethod
    def _schedule_route_registration(cls):
        if cls._BALANCE_ROUTE_TIMER is not None and cls._BALANCE_ROUTE_TIMER.is_alive():
            return

        def _retry():
            cls._BALANCE_ROUTE_TIMER = None
            cls.ensure_balance_route()

        timer = threading.Timer(1.0, _retry)
        timer.daemon = True
        cls._BALANCE_ROUTE_TIMER = timer
        timer.start()

    @staticmethod
    def _parse_bool(value: Optional[str]) -> bool:
        if value is None:
            return False
        return value.lower() in {"1", "true", "yes", "on"}

    @classmethod
    def ensure_balance_route(cls):
        if cls._BALANCE_ROUTE_REGISTERED:
            return
        prompt_server = getattr(PromptServer, "instance", None)
        if prompt_server is None:
            cls._schedule_route_registration()
            return

        @prompt_server.routes.get("/banana/token_usage")
        async def handle_token_usage(request):
            base_url = request.rel_url.query.get("base_url", cls.DEFAULT_API_BASE_URL)
            refresh = cls._parse_bool(request.rel_url.query.get("refresh"))
            # ä¼˜å…ˆä½¿ç”¨å‰ç«¯ä¼ é€’çš„API Key,å¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„Key
            api_key_from_request = request.rel_url.query.get("api_key", "").strip()
            api_key = cls._sanitize_api_key(api_key_from_request) or cls._sanitize_api_key(cls.load_config())
            cost_factor = cls.load_cost_factor_from_config()
            # è¿è¡Œäº aiohttp handler ä¸Šä¸‹æ–‡,ä¼˜å…ˆä½¿ç”¨è¿è¡Œä¸­çš„ loop
            loop = asyncio.get_running_loop()

            if not refresh:
                snapshot = cls._get_balance_snapshot(base_url, api_key)
                if snapshot is None:
                    return web.json_response({
                        "success": False,
                        "message": "æš‚æ— ä½™é¢ç¼“å­˜ï¼Œè¯·ç‚¹å‡»â€œæŸ¥è¯¢ä½™é¢â€æŒ‰é’®åˆ·æ–°",
                        "cached": False,
                        "stale": True
                    })

                summary = cls.format_balance_summary(snapshot, cost_factor, include_stale_hint=True)
                return web.json_response({
                    "success": True,
                    "data": snapshot.get("payload", {}).get("data"),
                    "raw": snapshot.get("payload"),
                    "summary": summary,
                    "cost_factor": cost_factor,
                    "cached": True,
                    "stale": cls._is_balance_snapshot_stale(snapshot)
                })

            try:
                await loop.run_in_executor(
                    None,
                    partial(cls.fetch_token_usage, base_url, api_key)
                )
                snapshot = cls._get_balance_snapshot(base_url, api_key)
                if snapshot is None:
                    raise RuntimeError("ä½™é¢ç¼“å­˜æ›´æ–°å¤±è´¥")
                summary = cls.format_balance_summary(snapshot, cost_factor)
                return web.json_response({
                    "success": True,
                    "data": snapshot.get("payload", {}).get("data"),
                    "raw": snapshot.get("payload"),
                    "summary": summary,
                    "cost_factor": cost_factor,
                    "cached": False,
                    "stale": False
                })
            except Exception as exc:
                return web.json_response(
                    {"success": False, "message": str(exc)},
                    status=400
                )

        cls._BALANCE_ROUTE_REGISTERED = True

    @staticmethod
    def _format_number(value: Optional[float]) -> str:
        if value is None:
            return "-"
        if isinstance(value, (int, float)):
            return f"{value:,.0f}"
        return str(value)

    @classmethod
    def _format_cost(cls, tokens: Optional[float], cost_factor: float) -> str:
        if tokens is None:
            return "-"
        try:
            tokens_value = float(tokens)
        except (TypeError, ValueError):
            return "-"
        yuan = tokens_value * cls.BASE_COST_PER_TOKEN * cost_factor
        return f"Â¥{yuan:.4f}"

    @classmethod
    def _format_expiry(cls, timestamp: Optional[int]) -> str:
        if not timestamp or timestamp <= 0:
            return "ä¸è¿‡æœŸ"
        try:
            dt = datetime.fromtimestamp(timestamp)
            return dt.strftime("%Y-%m-%d %H:%M")
        except Exception:
            return str(timestamp)

    @classmethod
    def format_balance_summary(cls, snapshot: Dict[str, Any], cost_factor: float = 1.0,
                               include_stale_hint: bool = False) -> str:
        cost_factor = cls._clamp_cost_factor(cost_factor)
        data = snapshot.get("payload", {}).get("data", {})
        available = cls._format_number(data.get("total_available"))
        used = cls._format_number(data.get("total_used"))
        granted = cls._format_number(data.get("total_granted"))
        unlimited = "æ˜¯" if data.get("unlimited_quota") else "å¦"
        expires = cls._format_expiry(data.get("expires_at"))
        available_cost = cls._format_cost(data.get("total_available"), cost_factor)
        used_cost = cls._format_cost(data.get("total_used"), cost_factor)
        fetched_at = snapshot.get("fetched_at")
        if fetched_at:
            fetched_text = datetime.fromtimestamp(fetched_at).strftime("%H:%M")
        else:
            fetched_text = datetime.now().strftime("%H:%M")
        summary_lines = [
            f"ğŸ”‘ æŸ¥è¯¢æ—¶é—´ {fetched_text}",
            f"ä¼°ç®—è´¹ç”¨: å¯ç”¨ {available_cost} / å·²ç”¨ {used_cost} (ä»…å‚è€ƒ)",
            f"åˆ°æœŸ: {expires}"
        ]
        if include_stale_hint and cls._is_balance_snapshot_stale(snapshot):
            age = cls._snapshot_age_seconds(snapshot)
            if age is not None:
                summary_lines.append(
                    f"âš ï¸ ä½™é¢ä¿¡æ¯å·² {int(age)}s æœªåˆ·æ–°ï¼Œç‚¹å‡»èŠ‚ç‚¹æŒ‰é’®è·å–æœ€æ–°æ•°æ®"
                )
        return "\n".join(summary_lines)

    @classmethod
    def get_cached_balance_text(cls, api_base_url: str, api_key: str, cost_factor: float = 1.0) -> Optional[str]:
        snapshot = cls._get_balance_snapshot(api_base_url, api_key)
        if not snapshot:
            return None
        try:
            return cls.format_balance_summary(snapshot, cost_factor, include_stale_hint=True)
        except Exception:
            return None

    @classmethod
    def _get_shared_session(cls) -> requests.Session:
        """
        è·å–å…±äº«çš„ HTTP Sessionï¼ˆè¿æ¥æ± å¤ç”¨ï¼‰
        ç®€åŒ–ä¸ºå•æ¬¡æ£€æŸ¥ï¼Œé¿å…åŒé‡æ£€æŸ¥é”å®šï¼ˆDCLï¼‰çš„æ½œåœ¨ç«æ€é—®é¢˜
        """
        with cls._SESSION_LOCK:
            if cls._SESSION is None:
                pool_size = max(4, cls.load_max_workers_from_config())
                session = requests.Session()
                adapter = HTTPAdapter(pool_connections=pool_size, pool_maxsize=pool_size)
                session.mount("http://", adapter)
                session.mount("https://", adapter)
                cls._SESSION = session
            return cls._SESSION

    @classmethod
    def fetch_token_usage(cls, api_base_url: str, api_key: str, timeout: int = 15) -> Dict[str, Any]:
        sanitized_key = cls._sanitize_api_key(api_key)
        if not sanitized_key:
            raise ValueError("æœªé…ç½®æœ‰æ•ˆçš„ API Key")
        base_url = (api_base_url or cls.DEFAULT_API_BASE_URL).rstrip("/")
        url = f"{base_url}/api/usage/token"
        headers = {"Authorization": f"Bearer {sanitized_key}"}
        session = cls._get_shared_session()
        # ç›´æ¥å‘é€è¯·æ±‚,Sessionä¼šè‡ªåŠ¨ç®¡ç†è¿æ¥æ± 
        response = session.get(url, headers=headers, timeout=timeout)
        try:
            try:
                response.raise_for_status()
            except requests.HTTPError as exc:
                raise RuntimeError(f"ä½™é¢æŸ¥è¯¢å¤±è´¥: HTTP {response.status_code}") from exc
            try:
                payload = response.json()
            except json.JSONDecodeError as exc:
                raise RuntimeError("ä½™é¢æŸ¥è¯¢å¤±è´¥: å“åº”é JSON") from exc
        finally:
            # ç¡®ä¿å“åº”å†…å®¹è¢«å®Œå…¨è¯»å–,è¿æ¥æ‰èƒ½è¢«å¤ç”¨
            response.close()
        cls._store_balance_snapshot(api_base_url, sanitized_key, payload)
        return payload

    @staticmethod
    def _get_config_path() -> str:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        return os.path.join(current_dir, "config.ini")

    @classmethod
    def load_config(cls):
        """ä»config.iniåŠ è½½API key"""
        config_path = cls._get_config_path()
        
        config = configparser.ConfigParser()
        
        # é»˜è®¤API key
        default_api_key = "your-api-key-here"
        
        # å°è¯•è¯»å–é…ç½®æ–‡ä»¶
        if os.path.exists(config_path):
            try:
                config.read(config_path, encoding='utf-8')
                if config.has_section('gemini'):
                    return config.get('gemini', 'api_key', fallback=default_api_key)
            except Exception as e:
                logger.warning(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        else:
            # åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶
            try:
                cpu_limit = max(1, os.cpu_count() or 4)
                default_workers = min(8, cpu_limit)
                config['gemini'] = {
                    'api_key': 'your-api-key-here',
                    'balance_cost_factor': '1.0',
                    'max_workers': str(default_workers)
                }
                with open(config_path, 'w', encoding='utf-8') as f:
                    config.write(f)
                logger.success(f"å·²åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶: {config_path}")
                logger.info(f"è¯·ç¼–è¾‘æ–‡ä»¶å¹¶å¡«å…¥ä½ çš„ API Key")
            except Exception as e:
                logger.warning(f"åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        
        return default_api_key

    @classmethod
    def load_cost_factor_from_config(cls) -> float:
        config_path = cls._get_config_path()
        config = configparser.ConfigParser()
        if os.path.exists(config_path):
            try:
                config.read(config_path, encoding="utf-8")
                if config.has_section('gemini'):
                    value = config.getfloat('gemini', 'balance_cost_factor', fallback=1.0)
                    return cls._clamp_cost_factor(value)
            except Exception as e:
                logger.warning(f"è¯»å– config ä¸­çš„ balance_cost_factor å¤±è´¥: {e}")
        return 1.0

    @classmethod
    def load_max_workers_from_config(cls) -> int:
        cpu_limit = max(1, os.cpu_count() or 1)
        default_workers = min(8, cpu_limit)
        config_path = cls._get_config_path()
        config = configparser.ConfigParser()
        if os.path.exists(config_path):
            try:
                config.read(config_path, encoding="utf-8")
                if config.has_section('gemini'):
                    value = config.getint('gemini', 'max_workers', fallback=default_workers)
                    return max(1, min(value, cpu_limit))
            except Exception as e:
                logger.warning(f"è¯»å– config ä¸­çš„ max_workers å¤±è´¥: {e}")
        return default_workers
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "prompt": ("STRING", {
                    "multiline": True,
                    "default": "Peace and love"
                }),
                "api_base_url": ("STRING", {
                    "default": "https://xinbaoapi.feng1994.xin"
                }),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False
                }),
                "model_type": ("STRING", {
                    "default": "gemini-2.5-flash-image"
                }),
                "batch_size": ("INT", {
                    "default": 1, "min": 1, "max": 8
                }),
                "aspect_ratio": (["Auto", "1:1", "9:16", "16:9", "21:9", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4"], {
                    "default": "Auto"
                }),
            },
            "optional": {
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 102400,
                    "control_after_generate": True
                }),
                "top_p": ("FLOAT", {
                    "default": 0.95, "min": 0.0, "max": 1.0, "step": 0.01
                }),
                "image_1": ("IMAGE",),
                "image_2": ("IMAGE",),
                "image_3": ("IMAGE",),
                "image_4": ("IMAGE",),
                "image_5": ("IMAGE",),
            }
        }
    
    def tensor_to_base64(self, tensor: torch.Tensor) -> str:
        """å°†tensorè½¬æ¢ä¸ºbase64"""
        img_array = (tensor[0].cpu().numpy() * 255).astype(np.uint8)
        img = Image.fromarray(img_array)
        buffered = BytesIO()
        img.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()

    def prepare_input_images(self, tensors: List[torch.Tensor]) -> List[str]:
        """å°†è¾“å…¥tensoré¢„ç¼–ç ä¸ºBase64å¹¶å¤ç”¨ç¼“å­˜"""
        if not tensors:
            return []
        encoded_images: List[str] = []
        for tensor in tensors:
            if tensor is None:
                continue
            cache_key = self._tensor_cache_key(tensor)
            cached_value = self._get_cached_image_b64(cache_key)
            if cached_value is None:
                base64_value = self.tensor_to_base64(tensor)
                self._set_cached_image_b64(cache_key, base64_value)
            else:
                base64_value = cached_value
            encoded_images.append(base64_value)
        return encoded_images

    def base64_to_tensor_single(self, b64_str: str) -> np.ndarray:
        """å°†å•ä¸ªbase64è½¬æ¢ä¸ºnumpyæ•°ç»„"""
        try:
            img_data = base64.b64decode(b64_str)
            img = Image.open(BytesIO(img_data)).convert('RGB')
            img_array = np.array(img).astype(np.float32) / 255.0
            return img_array
        except Exception as e:
            logger.error(f"å›¾ç‰‡è§£ç å¤±è´¥: {str(e)}")
            # è¿”å›ä¸€ä¸ªå°çš„é”™è¯¯å ä½å›¾
            return np.zeros((64, 64, 3), dtype=np.float32)

    def base64_to_tensor_parallel(self, base64_strings: List[str],
                                  log_prefix: Optional[str] = None,
                                  max_workers: Optional[int] = None) -> torch.Tensor:
        """å¹¶å‘è§£ç å¤šå¼ å›¾ç‰‡,å¯é€‰è‡ªå®šä¹‰æ—¥å¿—å‰ç¼€"""
        # å®‰å…¨çš„åˆ—è¡¨ç©ºå€¼æ£€æŸ¥,é¿å…tensorå¸ƒå°”å€¼æ­§ä¹‰
        if not isinstance(base64_strings, list) or len(base64_strings) == 0:
            return torch.zeros((1, 64, 64, 3), dtype=torch.float32)
        
        decode_start = time.time()
        images = []
        worker_cap = max_workers if max_workers is not None else max(4, os.cpu_count() or 1)
        worker_cap = max(1, worker_cap)
        effective_workers = min(worker_cap, len(base64_strings))

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è§£ç 
        self._ensure_not_interrupted()
        executor = ThreadPoolExecutor(max_workers=effective_workers)
        try:
            future_to_index = {executor.submit(self.base64_to_tensor_single, b64): i 
                             for i, b64 in enumerate(base64_strings)}
            
            # æŒ‰é¡ºåºæ”¶é›†ç»“æœ
            results = [None] * len(base64_strings)
            try:
                for future in as_completed(future_to_index):
                    index = future_to_index[future]
                    try:
                        self._ensure_not_interrupted()
                        results[index] = future.result()
                    except comfy.model_management.InterruptProcessingException:
                        # æ£€æµ‹åˆ°ä¸­æ–­ï¼Œç«‹å³å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                        for pending in future_to_index:
                            pending.cancel()
                        raise
                    except Exception as e:
                        logger.error(f"å›¾ç‰‡{index+1}è§£ç å¼‚å¸¸: {str(e)}")
                        results[index] = np.zeros((64, 64, 3), dtype=np.float32)

                images = [r for r in results if r is not None]
            except comfy.model_management.InterruptProcessingException:
                # ç¡®ä¿åœ¨ä¸­æ–­æ—¶å…³é—­çº¿ç¨‹æ± 
                executor.shutdown(wait=False, cancel_futures=True)
                raise
        finally:
            # ç¡®ä¿çº¿ç¨‹æ± è¢«å…³é—­
            if not executor._shutdown:
                executor.shutdown(wait=False, cancel_futures=True)

        decode_time = time.time() - decode_start
        logger.success(f"å¹¶å‘è§£ç  {len(images)} å¼ å›¾ç‰‡å®Œæˆï¼Œè€—æ—¶: {decode_time:.2f}s")

        return torch.from_numpy(np.stack(images))

    def _build_preview_tuple(self, tensor: Optional[torch.Tensor], batch_index: int,
                              max_size: int = 512) -> Optional[Tuple[str, Image.Image, int]]:
        """å°†ç”Ÿæˆç»“æœè½¬æ¢ä¸º ComfyUI æ‰€éœ€çš„å®æ—¶é¢„è§ˆæ ¼å¼"""
        if tensor is None or tensor.shape[0] == 0:
            return None

        try:
            preview_tensor = tensor[0].detach().cpu()
            preview_tensor = torch.clamp(preview_tensor, 0.0, 1.0)
            preview_array = (preview_tensor.numpy() * 255).astype(np.uint8)

            # å…¼å®¹å•é€šé“/Alpha é€šé“è¾“å‡º
            if preview_array.ndim == 3 and preview_array.shape[2] == 1:
                preview_array = np.repeat(preview_array, 3, axis=2)
            elif preview_array.ndim == 2:
                preview_array = np.stack([preview_array] * 3, axis=2)

            preview_image = Image.fromarray(preview_array)
            return ("PNG", preview_image, max_size)
        except Exception as e:
            logger.error(f"å®æ—¶é¢„è§ˆç”Ÿæˆå¤±è´¥: æ‰¹æ¬¡ {batch_index + 1}: {str(e)[:80]}")
            return None

    @staticmethod
    def _ensure_not_interrupted():
        """ç»Ÿä¸€çš„ä¸­æ–­æ£€æŸ¥ï¼Œå¤ç”¨ ComfyUI åŸç”Ÿå–æ¶ˆæœºåˆ¶"""
        comfy.model_management.throw_exception_if_processing_interrupted()

    def build_error_image_tensor(self, title: str, lines: List[str], size: Tuple[int, int] = (640, 640)) -> torch.Tensor:
        lines = [line.strip() for line in lines if line and line.strip()]
        if not lines:
            lines = ["å‘ç”ŸæœªçŸ¥é”™è¯¯"]

        width, height = size
        background = (248, 248, 248)
        accent = (255, 235, 235)
        title_color = (180, 30, 30)
        text_color = (45, 45, 45)

        img = Image.new("RGB", (width, height), background)
        draw = ImageDraw.Draw(img)
        font_title = self._load_error_font(26)
        font_body = self._load_error_font(18)

        margin = 32
        y = margin
        max_text_width = max(10, width - 2 * margin)
        max_y = height - margin

        def line_height(font: ImageFont.ImageFont) -> int:
            if hasattr(font, "getmetrics"):
                ascent, descent = font.getmetrics()
                return ascent + descent + 4
            if hasattr(font, "size"):
                return font.size + 4
            return 20

        title_text = title.strip() if title else "é”™è¯¯æç¤º"
        title_segments = self._wrap_text_segments(draw, title_text or "é”™è¯¯æç¤º", font_title, max_text_width) or ["é”™è¯¯æç¤º"]
        title_line_height = line_height(font_title)
        block_top = y - 6
        block_bottom = y + len(title_segments) * title_line_height + 6
        draw.rounded_rectangle([(margin - 10, block_top), (width - margin + 10, block_bottom)], radius=12, fill=accent)

        for segment in title_segments:
            draw.text((margin, y), segment, font=font_title, fill=title_color)
            y += title_line_height

        y += 6
        body_line_height = line_height(font_body)
        stop_render = False

        for line in lines:
            if stop_render:
                break
            segments = self._wrap_text_segments(draw, line, font_body, max_text_width) or [""]
            for segment in segments:
                if y > max_y:
                    stop_render = True
                    break
                draw.text((margin, y), segment, font=font_body, fill=text_color)
                y += body_line_height
            if stop_render:
                break
            y += 4

        arr = np.array(img).astype(np.float32) / 255.0
        return torch.from_numpy(arr).unsqueeze(0)

    def build_error_tensor_from_text(self, title: str, text: str) -> torch.Tensor:
        normalized = text.replace("\r\n", "\n").replace("\r", "\n")
        lines = [line.strip() for line in normalized.split("\n") if line.strip()]
        if not lines:
            lines = ["å‘ç”ŸæœªçŸ¥é”™è¯¯"]
        return self.build_error_image_tensor(title, lines)

    @classmethod
    def _get_error_font_paths(cls) -> List[str]:
        candidates = []
        windir = os.environ.get("WINDIR")
        if windir:
            for name in ("msyh.ttc", "msyh.ttf", "msjh.ttc", "simhei.ttf", "msmincho.ttc"):
                candidates.append(os.path.join(windir, "Fonts", name))
        candidates.append(os.path.join(os.path.dirname(__file__), "msyh.ttc"))
        return candidates

    @classmethod
    def _load_error_font(cls, size: int) -> ImageFont.ImageFont:
        cached = cls._ERROR_FONT_CACHE.get(size)
        if cached is not None:
            return cached
        for font_path in cls._get_error_font_paths():
            if font_path and os.path.exists(font_path):
                try:
                    font = ImageFont.truetype(font_path, size)
                    cls._ERROR_FONT_CACHE[size] = font
                    return font
                except Exception:
                    continue
        fallback = ImageFont.load_default()
        cls._ERROR_FONT_CACHE[size] = fallback
        return fallback

    def _wrap_text_segments(self, draw: ImageDraw.ImageDraw, text: str,
                            font: ImageFont.ImageFont, max_width: int) -> List[str]:
        if not text:
            return [""]
        segments: List[str] = []
        current = ""
        for ch in text:
            tentative = current + ch
            if draw.textlength(tentative, font=font) <= max_width or not current:
                current = tentative
            else:
                segments.append(current)
                current = ch
        if current:
            segments.append(current)
        return segments

    def create_request_data(self, prompt: str, seed: int, aspect_ratio: str,
                          top_p: float = 0.65, input_images_b64: Optional[List[str]] = None) -> Dict:
        """æ„å»ºè¯·æ±‚æ•°æ®"""
        if seed != -1:
            style_variations = [
                "detailed, high quality",
                "masterpiece, ultra detailed", 
                "photorealistic, stunning",
                "artistic, beautiful composition",
                "vibrant colors, sharp focus"
            ]
            style = style_variations[seed % len(style_variations)]
            final_prompt = f"{prompt}, {style}"
        else:
            final_prompt = prompt
            
        parts = [{"text": final_prompt}]
        
        if input_images_b64:
            for base64_image in input_images_b64:
                if base64_image:
                    parts.append({
                        "inlineData": {
                            "mimeType": "image/png",
                            "data": base64_image
                        }
                    })
        
        generation_config = {
            "responseModalities": ["IMAGE"],
            "temperature": 0.8,
            "topP": top_p,  # æ·»åŠ top_på‚æ•°
            "maxOutputTokens": 8192,
        }
        
        if aspect_ratio and aspect_ratio != "Auto":
            generation_config["imageConfig"] = {
                "aspectRatio": aspect_ratio
            }
        
        if seed != -1:
            generation_config["seed"] = seed
        
        return {
            "contents": [{
                "role": "user", 
                "parts": parts
            }],
            "generationConfig": generation_config
        }

    @retry_with_backoff(tries=3, delay=2, backoff=2)

    def send_request(self, api_key: str, request_data: Dict, model_type: str, 
                    api_base_url: str, timeout = 180) -> Dict:
        """å‘é€APIè¯·æ±‚"""
        endpoint = "generateContent"
        
        if "generativelanguage.googleapis.com" in api_base_url:
            url = f"{api_base_url.rstrip('/')}/v1beta/models/{model_type}:{endpoint}?key={api_key}"
            headers = {'Content-Type': 'application/json'}
        else:
            url = f"{api_base_url.rstrip('/')}/v1beta/models/{model_type}:{endpoint}"
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {api_key}',
            }
        
        headers['User-Agent'] = 'ComfyUI-Gemini-Node/2.1'

        try:
            session = self._get_shared_session()
            # ç›´æ¥å‘é€è¯·æ±‚,Sessionä¼šè‡ªåŠ¨ç®¡ç†è¿æ¥æ± 
            response = session.post(url, json=request_data, headers=headers, timeout=timeout)
            try:
                if response.status_code != 200:
                    # è¯»å–å°‘é‡æ–‡æœ¬ç”¨äºè¯Šæ–­
                    raise Exception(f"APIè¿”å› {response.status_code}: {response.text[:200]}")
                return response.json()
            finally:
                # ç¡®ä¿å“åº”å†…å®¹è¢«å®Œå…¨è¯»å–,è¿æ¥æ‰èƒ½è¢«å¤ç”¨
                response.close()
        except requests.exceptions.Timeout:
            raise Exception(f"è¯·æ±‚è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰")
        except requests.exceptions.RequestException as e:
            raise Exception(f"ç½‘ç»œé”™è¯¯: {str(e)}")

    def extract_content(self, response_data: Dict) -> Tuple[List[str], str]:
        """æå–å“åº”ä¸­çš„å›¾åƒå’Œæ–‡æœ¬"""
        base64_images = []
        text_content = ""
        
        candidates = response_data.get('candidates', [])
        if not candidates:
            raise ValueError("APIå“åº”ä¸­æ²¡æœ‰candidateså­—æ®µ")
        
        content = candidates[0].get('content', {})
        
        if content is None or content.get('parts') is None:
            return base64_images, text_content
        
        parts = content.get('parts', [])
        
        for part in parts:
            if 'text' in part:
                text_content += part['text']
            elif 'inlineData' in part and 'data' in part['inlineData']:
                base64_images.append(part['inlineData']['data'])
        
        if not base64_images and text_content:
            patterns = [
                r'data:image/[^;]+;base64,([A-Za-z0-9+/=]+)',
                r'!\[.*?\]\(data:image/[^;]+;base64,([A-Za-z0-9+/=]+)\)',
            ]
            for pattern in patterns:
                matches = re.findall(pattern, text_content)
                if matches:
                    base64_images.extend(matches)

        return base64_images, text_content.strip()

    def generate_single_image(self, args):
        """ç”Ÿæˆå•å¼ å›¾ç‰‡ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
        (
            i,
            current_seed,
            api_key,
            prompt,
            model_type,
            aspect_ratio,
            top_p,
            input_images_b64,
            api_base_url,
            timeout,
            stagger_delay,
            decode_workers,
        ) = args

        self._ensure_not_interrupted()
        if stagger_delay > 0:
            delay = i * stagger_delay
            if delay > 0:
                time.sleep(delay)

        thread_id = threading.current_thread().name
        logger.info(f"æ‰¹æ¬¡ {i+1} å¼€å§‹è¯·æ±‚...")

        try:
            self._ensure_not_interrupted()
            request_data = self.create_request_data(prompt, current_seed, aspect_ratio, top_p, input_images_b64)
            self._ensure_not_interrupted()
            response_data = self.send_request(api_key, request_data, model_type, api_base_url, timeout)
            self._ensure_not_interrupted()
            base64_images, text_content = self.extract_content(response_data)
            decoded_tensor = None
            decoded_count = 0
            if base64_images:
                self._ensure_not_interrupted()
                decoded_tensor = self.base64_to_tensor_parallel(
                    base64_images,
                    log_prefix=f"[{thread_id}] æ‰¹æ¬¡ {i+1}",
                    max_workers=decode_workers
                )
                decoded_count = decoded_tensor.shape[0]

            logger.success(f"æ‰¹æ¬¡ {i+1} å®Œæˆ - ç”Ÿæˆ {decoded_count} å¼ å›¾ç‰‡")

            return {
                'index': i,
                'success': True,
                'images': base64_images,
                'tensor': decoded_tensor,
                'image_count': decoded_count,
                'text': text_content,
                'seed': current_seed
            }
        except comfy.model_management.InterruptProcessingException:
            logger.warning(f"æ‰¹æ¬¡ {i+1} å·²å–æ¶ˆ")
            raise
        except Exception as e:
            error_msg = str(e)[:200]
            logger.error(f"æ‰¹æ¬¡ {i+1} å¤±è´¥")
            logger.error(f"é”™è¯¯: {error_msg}")
            return {
                'index': i,
                'success': False,
                'error': error_msg,
                'seed': current_seed,
                'tensor': None,
                'image_count': 0
            }

    def generate_images(self, prompt, api_base_url, api_key="", model_type="gemini-2.5-flash-image",
                       batch_size=1, aspect_ratio="Auto", seed=-1, top_p=0.95, max_workers=None,
                       image_1=None, image_2=None, image_3=None,
                       image_4=None, image_5=None):

        # è§£æ API Keyï¼šä¼˜å…ˆä½¿ç”¨èŠ‚ç‚¹è¾“å…¥ï¼Œç•™ç©ºæ—¶å›é€€ config
        sanitized_input_key = self._sanitize_api_key(api_key)
        resolved_api_key = sanitized_input_key or self._sanitize_api_key(self.load_config())

        # éªŒè¯API key
        if not resolved_api_key:
            error_msg = "è¯·åœ¨ config.ini ä¸­é…ç½® API Key æˆ–åœ¨èŠ‚ç‚¹ä¸­å¡«å†™"
            logger.error(error_msg)
            error_tensor = self.build_error_tensor_from_text(
                "é…ç½®ç¼ºå¤±",
                f"{error_msg}\nè¯·åœ¨ config.ini æˆ–èŠ‚ç‚¹è¾“å…¥ä¸­å¡«å†™æœ‰æ•ˆ API Key"
            )
            return (error_tensor, error_msg)

        cost_factor = self.load_cost_factor_from_config()
        balance_summary = self.get_cached_balance_text(api_base_url, resolved_api_key, cost_factor)

        start_time = time.time()
        raw_input_images = [image_1, image_2, image_3, image_4, image_5]
        input_tensors = [img for img in raw_input_images if img is not None]
        encoded_input_images = self.prepare_input_images(input_tensors)

        # å›ºå®šé…ç½®
        concurrent_mode = True  # æ€»æ˜¯å¼€å¯å¹¶å‘
        stagger_delay = 0.0     # ä¸ä½¿ç”¨äº¤é”™å»¶è¿Ÿ
        # æ‹†åˆ†ç½‘ç»œè¶…æ—¶ï¼šè¿æ¥(10s) + è¯»å–(60s)
        connect_timeout = 10
        read_timeout = 60
        request_timeout = (connect_timeout, read_timeout)
        continue_on_error = True  # æ€»æ˜¯å®¹é”™
        configured_workers = self.load_max_workers_from_config()
        decode_workers = max(1, configured_workers)

        if seed == -1:
            base_seed = random.randint(0, 102400)
        else:
            base_seed = seed

        decoded_tensors: List[torch.Tensor] = []
        total_generated_images = 0
        all_texts: List[str] = []
        results: List[Dict[str, Any]] = []
        tasks: List[Tuple[Any, ...]] = []

        for i in range(batch_size):
            current_seed = base_seed + i if seed != -1 else -1
            tasks.append((i, current_seed, resolved_api_key, prompt, model_type, aspect_ratio,
                          top_p, encoded_input_images, api_base_url, request_timeout, stagger_delay,
                          decode_workers))

        # æ˜¾ç¤ºä»»åŠ¡å¼€å§‹ä¿¡æ¯
        logger.header("ğŸ¨ Gemini å›¾åƒç”Ÿæˆä»»åŠ¡")
        logger.info(f"æ‰¹æ¬¡æ•°é‡: {batch_size} å¼ ")
        logger.info(f"å›¾ç‰‡æ¯”ä¾‹: {aspect_ratio}")
        if seed != -1:
            logger.info(f"éšæœºç§å­: {seed}")
        if top_p != 0.95:
            logger.info(f"Top-P å‚æ•°: {top_p}")
        logger.separator()

        # åˆ›å»º ComfyUI è¿›åº¦æ¡ - ä¼šåŒæ—¶åœ¨ Web UI å’Œæ§åˆ¶å°æ˜¾ç¤º
        pbar = comfy.utils.ProgressBar(batch_size)
        self._ensure_not_interrupted()

        used_concurrency = concurrent_mode and batch_size > 1
        completed = 0

        if used_concurrency:
            # å¯¹ç½‘ç»œå¹¶å‘åšæ›´ä¿å®ˆé™æµï¼Œé™ä½è¿œç«¯æŠ–åŠ¨æ—¶çš„è¿é”é˜»å¡æ¦‚ç‡
            network_workers_cap = min(configured_workers, 4)
            actual_workers = min(network_workers_cap, batch_size)
            # æ‰‹åŠ¨ç®¡ç†çº¿ç¨‹æ± ï¼Œé¿å…åœ¨è¶…æ—¶åœºæ™¯ä¸‹å›  wait=True é˜»å¡é€€å‡º
            executor = ThreadPoolExecutor(max_workers=actual_workers)
            try:
                future_to_index = {executor.submit(self.generate_single_image, task): task[0]
                                   for task in tasks}
                # è®¡ç®—æ€»ä½“ç­‰å¾…æ—¶é—´ï¼ˆè¿æ¥+è¯»å–+ä½™é‡ï¼‰ï¼Œé¿å…å•æ¬¡å¡ä½æ‹–ç´¯æ•´ä½“
                overall_timeout = connect_timeout + read_timeout + 30
                try:
                    for future in as_completed(future_to_index, timeout=overall_timeout):
                        try:
                            self._ensure_not_interrupted()
                            # as_completed å·²ä¿è¯å®Œæˆï¼Œæ— éœ€å†åŠ  result è¶…æ—¶
                            result = future.result()
                            results.append(result)
                            completed += 1
                            # æ˜¾ç¤ºæ‰¹æ¬¡å®Œæˆè¿›åº¦
                            if result['success']:
                                logger.success(f"[{completed}/{batch_size}] æ‰¹æ¬¡ {result['index']+1} å®Œæˆ")
                            else:
                                logger.error(f"[{completed}/{batch_size}] æ‰¹æ¬¡ {result['index']+1} å¤±è´¥")

                            # æ›´æ–° ComfyUI è¿›åº¦æ¡ï¼ˆå®æ—¶é¢„è§ˆï¼‰
                            preview_tensor = result.get('tensor')
                            if result.get('success') and preview_tensor is not None:
                                preview_tuple = self._build_preview_tuple(preview_tensor, result['index'])
                                if preview_tuple is not None:
                                    pbar.update_absolute(completed, batch_size, preview_tuple)
                                else:
                                    pbar.update(1)
                            else:
                                pbar.update(1)
                        except comfy.model_management.InterruptProcessingException:
                            # æ£€æµ‹åˆ°ä¸­æ–­ï¼Œå–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                            logger.warning(f"æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å–æ¶ˆå‰©ä½™ä»»åŠ¡...")
                            for pending in future_to_index:
                                pending.cancel()
                            raise
                        except Exception as e:
                            index = future_to_index.get(future, -1)
                            logger.error(f"æ‰¹æ¬¡ {index+1 if index>=0 else '?'} å¼‚å¸¸: {str(e)}")
                            results.append({
                                'index': index,
                                'success': False,
                                'error': str(e),
                                'tensor': None,
                                'image_count': 0
                            })
                except TimeoutError:
                    logger.warning(f"æ•´ä½“è¶…æ—¶ï¼å·²å®Œæˆ {completed}/{batch_size} ä¸ªä»»åŠ¡")
                    # å°è¯•å–æ¶ˆæœªå¼€å§‹çš„ä»»åŠ¡ï¼›å·²åœ¨è¿è¡Œçš„è¯·æ±‚æ— æ³•ç«‹å³å–æ¶ˆ
                    for future in future_to_index:
                        future.cancel()
                finally:
                    # å…³é”®ï¼šä¸ç­‰å¾…çº¿ç¨‹ç»“æŸä»¥å…å¡ä½ä¸»çº¿ç¨‹ï¼›è¿è¡Œä¸­çš„è¯·æ±‚ä¼šåœ¨åå°è‡ªè¡Œç»“æŸ
                    executor.shutdown(wait=False, cancel_futures=True)
            except Exception:
                executor.shutdown(wait=False, cancel_futures=True)
                raise
        else:
            for task in tasks:
                self._ensure_not_interrupted()
                result = self.generate_single_image(task)
                results.append(result)
                # æ˜¾ç¤ºæ‰¹æ¬¡å®Œæˆ
                if result['success']:
                    logger.success(f"[{task[0]+1}/{batch_size}] æ‰¹æ¬¡ {task[0]+1} å®Œæˆ")
                else:
                    logger.error(f"[{task[0]+1}/{batch_size}] æ‰¹æ¬¡ {task[0]+1} å¤±è´¥")

                # æ›´æ–° ComfyUI è¿›åº¦æ¡ï¼ˆå®æ—¶é¢„è§ˆï¼‰
                preview_tensor = result.get('tensor')
                if result.get('success') and preview_tensor is not None:
                    preview_tuple = self._build_preview_tuple(preview_tensor, task[0])
                    if preview_tuple is not None:
                        pbar.update_absolute(task[0]+1, batch_size, preview_tuple)
                    else:
                        pbar.update(1)
                else:
                    pbar.update(1)
                if not result['success'] and not continue_on_error:
                    logger.warning("é‡åˆ°é”™è¯¯ä¸”æœªå¼€å¯å®¹é”™ï¼Œåœæ­¢å¤„ç†")
                    break

        if not results:
            error_text = f"æœªç”Ÿæˆä»»ä½•å›¾åƒ\næ€»è€—æ—¶: {time.time() - start_time:.2f}s"
            if balance_summary:
                error_text = f"{balance_summary}\n\n{error_text}"
            logger.error(error_text)
            error_tensor = self.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", error_text)
            return (error_tensor, error_text)

        results.sort(key=lambda x: x['index'])

        for result in results:
            if result.get('success'):
                tensor = result.get('tensor')
                if tensor is not None:
                    decoded_tensors.append(tensor)
                    total_generated_images += result.get('image_count', tensor.shape[0])
                if result.get('text'):
                    all_texts.append(f"[æ‰¹æ¬¡ {result['index']+1}] {result['text']}")
            else:
                error_msg = f"[æ‰¹æ¬¡ {result['index']+1}] âŒ {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                all_texts.append(error_msg)
                if not continue_on_error:
                    break

        total_time = time.time() - start_time

        if not decoded_tensors or total_generated_images == 0:
            error_text = f"æœªç”Ÿæˆä»»ä½•å›¾åƒ\næ€»è€—æ—¶: {total_time:.2f}s\n\n" + "\n".join(all_texts)
            if balance_summary:
                error_text = f"{balance_summary}\n\n{error_text}"
            logger.error(error_text)
            error_tensor = self.build_error_tensor_from_text("ç”Ÿæˆå¤±è´¥", error_text)
            return (error_tensor, error_text)

        if len(decoded_tensors) == 1:
            image_tensor = decoded_tensors[0]
        else:
            image_tensor = torch.cat(decoded_tensors, dim=0)

        actual_count = total_generated_images
        ratio_text = "è‡ªåŠ¨" if aspect_ratio == "Auto" else aspect_ratio
        success_info = f"âœ… æˆåŠŸç”Ÿæˆ {actual_count} å¼ å›¾åƒï¼ˆæ¯”ä¾‹: {ratio_text}ï¼‰"
        avg_time = total_time / actual_count if actual_count > 0 else 0
        time_info = f"æ€»è€—æ—¶: {total_time:.2f}sï¼Œå¹³å‡ {avg_time:.2f}s/å¼ "
        if actual_count != batch_size:
            time_info += f" âš ï¸ è¯·æ±‚{batch_size}å¼ ï¼Œå®é™…ç”Ÿæˆ{actual_count}å¼ "

        combined_text = f"{success_info}\n{time_info}"
        if all_texts:
            combined_text += "\n\n" + "\n".join(all_texts)
        if balance_summary:
            combined_text = f"{balance_summary}\n\n{combined_text}"

        # æ˜¾ç¤ºå®Œæˆç»Ÿè®¡
        logger.summary("ä»»åŠ¡å®Œæˆ", {
            "æ€»æ‰¹æ¬¡": f"{batch_size} ä¸ª",
            "æˆåŠŸç”Ÿæˆ": f"{actual_count} å¼ ",
            "æ€»è€—æ—¶": f"{total_time:.2f}s",
            "å¹³å‡é€Ÿåº¦": f"{avg_time:.2f}s/å¼ "
        })

        return (image_tensor, combined_text)

# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {"BananaImageNode": BananaImageNode}
NODE_DISPLAY_NAME_MAPPINGS = {"BananaImageNode": "å¿ƒå®â¤Banana"}

BananaImageNode.ensure_balance_route()
